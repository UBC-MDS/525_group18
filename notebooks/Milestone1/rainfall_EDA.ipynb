{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 525: Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing packages that are not included in the environment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!conda install --yes --prefix {sys.prefix} altair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage\n",
    "import dask.dataframe as dd\n",
    "import gc\n",
    "import sys\n",
    "import dask\n",
    "from dask.diagnostics import Profiler, ResourceProfiler, CacheProfiler\n",
    "import altair as alt\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import dask.dataframe as dd\n",
    "import pyarrow.feather as feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n",
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Team-work contract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Please find the link to the teamwork contract [here](https://docs.google.com/document/d/1Sfs2RBib0FiGaDob7pGoitGPthRL-F32-dZo7A-aOhg/edit#heading=h.tgeu41bizsao)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating repository and project structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Please find the GitHub repository for the project [here](https://github.com/UBC-MDS/525_group18)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Downloading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = 14096681\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"figshare_rainfall/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the necessary files from the figshareAPI for the rainfall article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_link_only': False,\n",
       "  'name': 'daily_rainfall_2014.png',\n",
       "  'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'id': 26579150,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "  'size': 58863},\n",
       " {'is_link_only': False,\n",
       "  'name': 'environment.yml',\n",
       "  'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'id': 26579171,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "  'size': 192},\n",
       " {'is_link_only': False,\n",
       "  'name': 'README.md',\n",
       "  'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'id': 26586554,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "  'size': 5422},\n",
       " {'is_link_only': False,\n",
       "  'name': 'data.zip',\n",
       "  'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'id': 26766812,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "  'size': 814041183},\n",
       " {'is_link_only': False,\n",
       "  'name': 'get_data.py',\n",
       "  'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'computed_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'id': 26766815,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "  'size': 4113}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  \n",
    "files = data[\"files\"]             \n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested to download the `data.zip` file from the rainfall article from figshare.\n",
    "\n",
    "We are also timing how long it takes to download the data through the figshareAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.54 s, sys: 6.98 s, total: 15.5 s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files_to_dl = [\"data.zip\"]  \n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the file from the `data.zip` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comibining the data CSVs with Pandas and Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 192.93 MiB, increment: 0.03 MiB\n",
      "CPU times: user 7min 4s, sys: 13.5 s, total: 7min 18s\n",
      "Wall time: 7min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "\n",
    "com_dat_path = \"figshare_rainfall/combined_data.csv\"\n",
    "\n",
    "files = glob.glob('figshare_rainfall/*NSW.csv')\n",
    "df = pd.concat((pd.read_csv(file, index_col=0)\n",
    "                .assign(model=re.findall(r'[^\\/]+(?=\\_d)', file)[0])\n",
    "                for file in files)\n",
    "              )\n",
    "df.to_csv(com_dat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6G\tfigshare_rainfall/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh figshare_rainfall/combined_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 22.4 s, total: 1min 38s\n",
      "Wall time: 6min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\"figshare_rainfall/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data Usage for combined dataframe in pandas is 3498199336\n"
     ]
    }
   ],
   "source": [
    "print(f\"The data Usage for combined dataframe in pandas is {df.memory_usage(index=True).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62467843 entries, 0 to 62467842\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   time           object \n",
      " 1   lat_min        float64\n",
      " 2   lat_max        float64\n",
      " 3   lon_min        float64\n",
      " 4   lon_max        float64\n",
      " 5   rain (mm/day)  float64\n",
      " 6   model          object \n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 3.3+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11448609766"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62467843, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-35.811518</td>\n",
       "      <td>-34.86911</td>\n",
       "      <td>140.625</td>\n",
       "      <td>141.875</td>\n",
       "      <td>3.045650e-13</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-35.811518</td>\n",
       "      <td>-34.86911</td>\n",
       "      <td>140.625</td>\n",
       "      <td>141.875</td>\n",
       "      <td>3.572392e-04</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-35.811518</td>\n",
       "      <td>-34.86911</td>\n",
       "      <td>140.625</td>\n",
       "      <td>141.875</td>\n",
       "      <td>9.431562e-03</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-35.811518</td>\n",
       "      <td>-34.86911</td>\n",
       "      <td>140.625</td>\n",
       "      <td>141.875</td>\n",
       "      <td>9.663865e-02</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-35.811518</td>\n",
       "      <td>-34.86911</td>\n",
       "      <td>140.625</td>\n",
       "      <td>141.875</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time    lat_min   lat_max  lon_min  lon_max  rain (mm/day)  \\\n",
       "0  1889-01-01 12:00:00 -35.811518 -34.86911  140.625  141.875   3.045650e-13   \n",
       "1  1889-01-02 12:00:00 -35.811518 -34.86911  140.625  141.875   3.572392e-04   \n",
       "2  1889-01-03 12:00:00 -35.811518 -34.86911  140.625  141.875   9.431562e-03   \n",
       "3  1889-01-04 12:00:00 -35.811518 -34.86911  140.625  141.875   9.663865e-02   \n",
       "4  1889-01-05 12:00:00 -35.811518 -34.86911  140.625  141.875   0.000000e+00   \n",
       "\n",
       "         model  \n",
       "0  SAM0-UNICON  \n",
       "1  SAM0-UNICON  \n",
       "2  SAM0-UNICON  \n",
       "3  SAM0-UNICON  \n",
       "4  SAM0-UNICON  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Observations***\n",
    "\n",
    "- Zhenrui：\n",
    "\n",
    "    - Combining files:\n",
    "        - peak memory: 2770.49 MiB, increment: 0.02 MiB\n",
    "        - Wall time: 6min 56s\n",
    "    - Reading combined file:\n",
    "        - Wall time: 1min 03s\n",
    "    - Combined Csv size\n",
    "        - 5.7G - figshare_rainfall/combined_data.csv\n",
    "    - System resource Usage:\n",
    "        - Ram: 5.8 GB\n",
    "        - Cache: 2.9 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bruhat:\n",
    "    - Combining files:\n",
    "        - peak memory: 154.38 MiB, increment: 0.00 MiB\n",
    "        - Wall time: 7min 38s\n",
    "    - Reading combined file:\n",
    "        - Wall time: 4min 6s\n",
    "    - Combined Csv size\n",
    "        - 5.6G - figshare_rainfall/combined_data.csv\n",
    "    - System resource Usage:\n",
    "        - Ram: 5.7 GB\n",
    "        - Cache: 2.9 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deepak: Panda's way of combining the different csv files did not work for me, my computer crushed while processing the code. Since I only have 4GB RAM available for memory.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 5328.51 MiB, increment: 0.84 MiB\n",
      "CPU times: user 7min 54s, sys: 18.7 s, total: 8min 12s\n",
      "Wall time: 7min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/bruhat/april_block/525_web_cloud_comp/525_group18/notebooks/figshare_rainfall/combined_data_dask.csv']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "%memit\n",
    "file_list = glob.glob('figshare_rainfall/*NSW.csv')\n",
    "\n",
    "def read_and_label_csv(filename):\n",
    "    '''\n",
    "    This function loads the csv file and adds a 'Model' column\n",
    "    based on the filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "        relative path to the file \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        df_csv, a dataframe\n",
    "    '''\n",
    "\n",
    "    # reads each csv file to a pandas.DataFrame\n",
    "    df_csv = pd.read_csv(filename, index_col=0)\n",
    "    df_csv['Model'] = re.findall(r'[^\\/]+(?=\\_d)',filename)[0]\n",
    "    return df_csv \n",
    "\n",
    "# create a list of functions ready to return a DataFrame\n",
    "dfs = [dask.delayed(read_and_label_csv)(fname) for fname in file_list]\n",
    "\n",
    "# using delayed, assemble the pandas.DataFrames into a dask.DataFrame\n",
    "ddf = dd.from_delayed(dfs)\n",
    "\n",
    "ddf.to_csv(\"figshare_rainfall/combined_data_dask.csv\", single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6G\tfigshare_rainfall/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh figshare_rainfall/combined_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.7 ms, sys: 78 µs, total: 25.8 ms\n",
      "Wall time: 53.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = dd.read_csv(\"figshare_rainfall/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-35.811518</td>\n",
       "      <td>-34.86911</td>\n",
       "      <td>140.625</td>\n",
       "      <td>141.875</td>\n",
       "      <td>3.045650e-13</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-35.811518</td>\n",
       "      <td>-34.86911</td>\n",
       "      <td>140.625</td>\n",
       "      <td>141.875</td>\n",
       "      <td>3.572392e-04</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-35.811518</td>\n",
       "      <td>-34.86911</td>\n",
       "      <td>140.625</td>\n",
       "      <td>141.875</td>\n",
       "      <td>9.431562e-03</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-35.811518</td>\n",
       "      <td>-34.86911</td>\n",
       "      <td>140.625</td>\n",
       "      <td>141.875</td>\n",
       "      <td>9.663865e-02</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-35.811518</td>\n",
       "      <td>-34.86911</td>\n",
       "      <td>140.625</td>\n",
       "      <td>141.875</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>SAM0-UNICON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time    lat_min   lat_max  lon_min  lon_max  rain (mm/day)  \\\n",
       "0  1889-01-01 12:00:00 -35.811518 -34.86911  140.625  141.875   3.045650e-13   \n",
       "1  1889-01-02 12:00:00 -35.811518 -34.86911  140.625  141.875   3.572392e-04   \n",
       "2  1889-01-03 12:00:00 -35.811518 -34.86911  140.625  141.875   9.431562e-03   \n",
       "3  1889-01-04 12:00:00 -35.811518 -34.86911  140.625  141.875   9.663865e-02   \n",
       "4  1889-01-05 12:00:00 -35.811518 -34.86911  140.625  141.875   0.000000e+00   \n",
       "\n",
       "         model  \n",
       "0  SAM0-UNICON  \n",
       "1  SAM0-UNICON  \n",
       "2  SAM0-UNICON  \n",
       "3  SAM0-UNICON  \n",
       "4  SAM0-UNICON  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time              object\n",
       "lat_min          float64\n",
       "lat_max          float64\n",
       "lon_min          float64\n",
       "lon_max          float64\n",
       "rain (mm/day)    float64\n",
       "model             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Observations***\n",
    "\n",
    "- Deepak：\n",
    "    - Combining files:\n",
    "        - peak memory: 175.45 MiB, increment: 0.37 MiB\n",
    "        - Wall time: 7min 44s\n",
    "    - Reading combined file:\n",
    "        - Wall time: 444 ms\n",
    "    - Combined Csv size\n",
    "        - 6.0G - figshare_rainfall/combined_data_dask.csv\n",
    "    - System resource Usage:\n",
    "        - Ram: 4.5 GB\n",
    "        - Cache: 1.5 GB\n",
    "\n",
    "- Zhenrui：\n",
    "    - Combining files:\n",
    "        - peak memory: 174.59 MiB, increment: 0.00 MiB\n",
    "        - Wall time: 7min 50s\n",
    "    - Reading combined file:\n",
    "        - Wall time: 22.7 ms\n",
    "    - Combined Csv size\n",
    "        - 6.7G - figshare_rainfall/combined_data_dask.csv\n",
    "    - System resource Usage:\n",
    "        - Ram: 4.5 GB\n",
    "        - Cache: 1.5 GB   \n",
    "        \n",
    "- Bruhat：\n",
    "    - Combining files:\n",
    "        - peak memory: 5328.51 MiB, increment: 0.84 MiB\n",
    "        - Wall time: 7min 20s\n",
    "    - Reading combined file:\n",
    "        - Wall time: 53.9 ms\n",
    "    - Combined Csv size\n",
    "        - 5.6G\tfigshare_rainfall/combined_data.csv\n",
    "    - System resource Usage:\n",
    "        - Ram: 5.5 GB\n",
    "        - Cache: 2.5 GB   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparision, dask obviosly has the faster read speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load the combined CSV to memory and perform a simple EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clearing out previous memory \n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Dask Approach to reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 190.88 MiB, increment: 0.02 MiB\n",
      "CPU times: user 60 ms, sys: 64.5 ms, total: 124 ms\n",
      "Wall time: 278 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "df_dask = dd.read_csv(com_dat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Series Structure:\n",
       "npartitions=1\n",
       "    int64\n",
       "      ...\n",
       "dtype: int64\n",
       "Dask Name: series-groupby-sum-agg, 391 tasks"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dask.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "TaiESM1             3541230\n",
      "NorESM2-MM          3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "MRI-ESM2-0          3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM5-0           1609650\n",
      "INM-CM4-8           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "Name: model, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_dask.model.value_counts().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 7 entries, time to model\n",
      "dtypes: object(2), float64(5)"
     ]
    }
   ],
   "source": [
    "df_dask.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Loading just columns what we want approach to reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 8849.23 MiB, increment: 0.00 MiB\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "\n",
    "#loading time and rain col to perform eda\n",
    "df_reqcol = pd.read_csv(com_dat_path, usecols = ['time', 'rain (mm/day)'])\n",
    "df_reqcol['time'] = pd.to_datetime(df_reqcol['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62467843 entries, 0 to 62467842\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Dtype         \n",
      "---  ------         -----         \n",
      " 0   time           datetime64[ns]\n",
      " 1   rain (mm/day)  float64       \n",
      "dtypes: datetime64[ns](1), float64(1)\n",
      "memory usage: 953.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_reqcol.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By using dask, due to the feature of multiprocessing or parrallel, it took much less time to read the data. Reading with pandas took 6min 4s where as in dask, it was only 278 ms. Initially, we tried reading the data and Changing dtype of the data. That did not give us any significant improvement in space saving.\n",
    "\n",
    "- The peak memory with dask was 190.88 MiB which was considerably lower than reading with pandas.\n",
    "\n",
    "- Loading in chunks also seemed like a viable option, but in practice turned out to increase complexity of code without significant increase in memory savings compared to dask.\n",
    "\n",
    "- If we are trying to stray away from dask, A viable alternative is to just load the columns that we need and perform the eda, which is an alternative we tried above. The memory usage was 953.2 MB which is far better than the size of whole data frame (5.6Gb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perform a simple EDA in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 6978.16 MiB, increment: 6631.93 MiB\n",
      "CPU times: user 1min 8s, sys: 15.3 s, total: 1min 24s\n",
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "#simple pandas - This is how we do normally ,which means we are loading the entire data to the memory\n",
    "# just loading 10 million rows\n",
    "df = pd.read_csv(\"figshare_rainfall/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# %%R -i df\n",
    "\n",
    "# start_time <- Sys.time()\n",
    "# library(dplyr)\n",
    "# result <- df %>% count(model)\n",
    "# end_time <- Sys.time()\n",
    "# print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***does not run in our computers (ran it for 2hrs with 18gb ram and it eventually just crashes)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Arrow + parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 4303.03 MiB, increment: 4112.61 MiB\n",
      "CPU times: user 29.1 s, sys: 3.77 s, total: 32.9 s\n",
      "Wall time: 30.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Read data file and prepare arrow table\n",
    "dataset = ds.dataset(\"figshare_rainfall/combined_data.csv\", format=\"csv\")\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 s, sys: 461 ms, total: 10.5 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pq.write_to_dataset(table, 'figshare_rainfall/combined_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6G\tfigshare_rainfall/combined_data.csv\n",
      "1.1G\tfigshare_rainfall/combined_data.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh figshare_rainfall/combined_data.csv\n",
    "du -sh figshare_rainfall/combined_data.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘arrow’\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from ‘package:utils’:\n",
      "\n",
      "    timestamp\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 27 x 2\u001b[39m\n",
      "   model                  n\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m ACCESS-CM2       1\u001b[4m9\u001b[24m\u001b[4m3\u001b[24m\u001b[4m2\u001b[24m840\n",
      "\u001b[90m 2\u001b[39m ACCESS-ESM1-5    1\u001b[4m6\u001b[24m\u001b[4m1\u001b[24m\u001b[4m0\u001b[24m700\n",
      "\u001b[90m 3\u001b[39m AWI-ESM-1-1-LR    \u001b[4m9\u001b[24m\u001b[4m6\u001b[24m\u001b[4m6\u001b[24m420\n",
      "\u001b[90m 4\u001b[39m BCC-CSM2-MR      3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m5\u001b[24m340\n",
      "\u001b[90m 5\u001b[39m BCC-ESM1          \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 6\u001b[39m CanESM5           \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 7\u001b[39m CMCC-CM2-HR4     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 8\u001b[39m CMCC-CM2-SR5     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 9\u001b[39m CMCC-ESM2        3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m10\u001b[39m EC-Earth3-Veg-LR 3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m7\u001b[24m320\n",
      "\u001b[90m# … with 17 more rows\u001b[39m\n",
      "Time difference of 56.10742 secs\n",
      "CPU times: user 11.7 s, sys: 7.96 s, total: 19.7 s\n",
      "Wall time: 57.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "\n",
    "library(arrow)\n",
    "start_time <- Sys.time()\n",
    "file_name <- list.files(path = \"figshare_rainfall/combined_data.parquet/\", pattern=\"\\\\.parquet$\")\n",
    "r_table <- arrow::read_parquet(paste(\"figshare_rainfall/combined_data.parquet/\", file_name, sep=\"\"))\n",
    "print(class(r_table))\n",
    "library(dplyr)\n",
    "result <- r_table %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Arrow + feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.56 s, sys: 1.54 s, total: 7.1 s\n",
      "Wall time: 9.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feather.write_feather(table, 'figshare_rainfall/combined_data.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1G\tfigshare_rainfall/combined_data.feather\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh figshare_rainfall/combined_data.feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 27 x 2\u001b[39m\n",
      "   model                  n\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m ACCESS-CM2       1\u001b[4m9\u001b[24m\u001b[4m3\u001b[24m\u001b[4m2\u001b[24m840\n",
      "\u001b[90m 2\u001b[39m ACCESS-ESM1-5    1\u001b[4m6\u001b[24m\u001b[4m1\u001b[24m\u001b[4m0\u001b[24m700\n",
      "\u001b[90m 3\u001b[39m AWI-ESM-1-1-LR    \u001b[4m9\u001b[24m\u001b[4m6\u001b[24m\u001b[4m6\u001b[24m420\n",
      "\u001b[90m 4\u001b[39m BCC-CSM2-MR      3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m5\u001b[24m340\n",
      "\u001b[90m 5\u001b[39m BCC-ESM1          \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 6\u001b[39m CanESM5           \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 7\u001b[39m CMCC-CM2-HR4     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 8\u001b[39m CMCC-CM2-SR5     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 9\u001b[39m CMCC-ESM2        3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m10\u001b[39m EC-Earth3-Veg-LR 3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m7\u001b[24m320\n",
      "\u001b[90m# … with 17 more rows\u001b[39m\n",
      "Time difference of 3.334692 mins\n",
      "CPU times: user 16 s, sys: 15.4 s, total: 31.4 s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "\n",
    "library(arrow)\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_feather(\"figshare_rainfall/combined_data.feather\")\n",
    "print(class(r_table))\n",
    "library(dplyr)\n",
    "result <- r_table %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the storage usage and time spending, arrow and parquet is the best option. Obviously, transfering the python dataframe to R is not an ideal way for the data with quite big size. We waited for over 20 minuties, there is still no result. Compared with the `csv` file, the feather and parquet both have the smaller size. However, our group found that it seems Windows cannot use feather file with arrow table. Normally, it only takes around 3 minutes to count the model. But the Windows cannot run the cell sccuessfully. On the contrary, parquet file with arrow table is the best option, which takes only 57.4s to count the model, and it can run properly both in Windows and Linux (we do not have Mac user in our group)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
